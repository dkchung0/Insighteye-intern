import numpy as np
import pandas as pd
import re
import arrow
import time

import requests
from bs4 import BeautifulSoup 

import pymongo
from pymongo import MongoClient


class tvbs_crawler:
    def __init__(self,date,header,connect,table) :
        self.date = date
        self.users = header
        self.database = connect["test"]
        self.table = table

    def get_web_page(self,url):

        w = requests.get(url,headers=self.users)                                                      
        w.encoding = "utf-8"
        
        if w.status_code != 200 :
            print("Invalid url")
            return None
        else :
            soup = BeautifulSoup(w.text,"html.parser")
            return soup

    def rule_page(self):

        url = "https://news.tvbs.com.tw/realtime/news/" + self.date

        news = self.get_web_page(url)

        realtime_url = []
        for i in range(len(news.select("div.news_list div.list a"))):
            if i%2 != 0 :
                continue
            realtime_url.append(news.select("div.news_list div.list a")[i].attrs["href"])

        return realtime_url
    
    def news_page(self,news):
    
        data = news
        
        insert_time = arrow.now()
        insert_time = insert_time.datetime

        field = "news"

        website = data.select("meta[property='dable:author']")[0].attrs["content"]

        url = data.select("meta[property='og:url']")[0]["content"]

        pattern = re.compile(r'(?<=ç™¼ä½ˆæ™‚é–“ï¼š)(\d+)/(\d+)/(\d+) (\d+):(\d+)')
        t = data.select("div.author")[0].text
        p = pattern.search(t)
        release_time = arrow.get(int(p.group(1)),int(p.group(2)),int(p.group(3)),int(p.group(4)),int(p.group(5)))
        release_time = release_time.datetime
        
        category = data.select("div.bread a")[-1].text

        #domain

        title = data.select("h1.title")[0].text

        try:
            author = data.select("div.author a")[0].text
        except:
            author = ""

        content = news.select("div#news_detail_div")[0].text
        content = re.sub("\nâ—¤FUNå¿ƒå‡ºéŠ\u3000ç²¾ç·»äº«å—â—¢ðŸ‘‰é£Ÿå°šçŽ©å®¶äººæ°£é£¯åº—å¿«é–ƒåœ˜åªæœ‰5å¤©ðŸ‘‰é«˜è³ªæ„Ÿç²¾å“é£¯åº—å…è²»å‡ç­‰è¶…åˆ’ç®—ðŸ‘‰ç”¨åœ‹æ—…åˆ¸ï¼è³ºæ˜Ÿå·´å…‹é›»å­é£²æ–™åˆ¸ðŸ‘‰å…¨å°å”¯ä¸€å¡é€šé »é“è¯åä¸»é¡Œæˆ¿ðŸ‘‰ç¶“å…¸ä¸»é¡Œé£¯åº—å¸¶ä½ é¨éŠæµ·æ´‹ä¸–ç•ŒðŸ‘‰è¨‚æˆ¿åŠ è³¼é«˜éµç¥¨é‚„æœ‰75æŠ˜å„ªæƒ  ","",content)
        content = re.sub("æœ€HOTè©±é¡Œåœ¨é€™ï¼æƒ³è·Ÿä¸Šæ™‚äº‹ï¼Œå¿«é»žæˆ‘åŠ å…¥TVBSæ–°èžLINEå¥½å‹ï¼","",content)
        content = re.sub("æŽŒæ¡å³æ™‚è·¯æ³ï¼é–ƒéŽåœ‹é“å¡žè»Šåœ°é›·è·¯æ®µ","",content)
        content = re.sub("\n","",content)
        content = re.sub("\t","",content)
        content = re.sub("\xa0","",content)

        #click_resp_cnt

        #resp_list
        
        #resp_cnt
        
        #volume
        
        tag = news.select("div.article_keyword")[0].text
        pattern = re.compile(r'(?<=\#)\w+')
        tag = pattern.findall(tag)


        return insert_time , field , website , url , release_time , category , title , author , content , tag

    def run_data(self):
        
        t1_start = time.time()

        realtime_url = self.rule_page()

        try:
            for i in realtime_url :
                url_ = "https://news.tvbs.com.tw" +  i
                news = self.get_web_page(url_)
                insert_time , field , website , url , release_time , category , title , author , content , tag =  self.news_page(news)
                self.database[self.table].insert_one({"insert_time":insert_time,
                                                      "field":field,
                                                      "website":website,
                                                      "url":url,
                                                      "time":release_time,
                                                      "category":category,
                                                      "domain":"",
                                                      "title":title,
                                                      "author":author,
                                                      "content":content,
                                                      "click_resp_cnt":0,
                                                      "resp_list":[],
                                                      "resp_cnt":0,
                                                      "volume":0,
                                                      "tag":[tag]})
        except:
            print(url_)


        t1_end = time.time()

        print("Elapsed time during the whole program in seconds:{}".format(t1_end-t1_start)) 


###################################################################################  2æœˆ1æ—¥ ~ 2æœˆ24æ—¥ (data)


for i in range(24):

    i+=1

    if i<10:
        date = "2022-02-0{}".format(i)
        table = "tvbs_2022-02-0{}".format(i)
    else :
        date = "2022-02-{}".format(i)
        table = "tvbs_2022-02-{}".format(i)

    header = {"user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36"}
    connect = MongoClient('localhost', 27017)


    crawler_ = tvbs_crawler(date,header,connect,table)
    crawler_.run_data()
                    